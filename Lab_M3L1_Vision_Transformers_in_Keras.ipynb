{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3057f4b9-bfce-4093-86a6-b8fb9b4702c5",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\">\n",
    "  </a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f987e3d7-707a-4548-8f09-c7bd5bbfd619",
   "metadata": {},
   "source": [
    "<h1 align=left><font size = 6>Lab: Vision Transformers Using Keras </font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5c1104-b4b0-427a-befb-1b205b14486c",
   "metadata": {},
   "source": [
    "<h5>Estimated time: 90 minutes</h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e38e483-8620-422f-a38b-4b5fe1244513",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, you will learn to build a CNN-Vision Transformer (ViT) hybrid image classification model. You will start by loading an existing CNN that is good at recognizing small patterns in pictures. Then, you'll learn how to improve it with a transformer, which helps the model see and use wider and more complex relationships in an image. The notebook covers important topics like preparing your image data, making your model smarter with both local and global learning, and saving your best results automatically. By the end, you'll understand how CNN-ViT hybrid models work and how to train, evaluate, and visualize them for any image classification task.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7955effd",
   "metadata": {},
   "source": [
    "<h2>Objective</h2>\n",
    "\n",
    "This notebook demonstrates how to use a custom-trained Keras CNN model to extract feature maps and feed them into a ViT architecture.\n",
    "\n",
    "After completing this lab, you will be able to:\n",
    "<ul>\n",
    "    \n",
    "1. Load the custom-trained CNN model\n",
    "2. Extract feature maps from the CNN\n",
    "3. Prepare tokens for the Vision Transformer\n",
    "4. Build the Vision Transformer encoder\n",
    "5. Train and evaluate the combined model\n",
    "\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08864f02",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "<font size = 3> \n",
    "    \n",
    "1. [ Custom positional embedding layer](#Custom-positional-embedding-layer)\n",
    "3. [Transformer block implementation](#Transformer-block-implementation)\n",
    "4. [Hybrid model builder function](#Hybrid-model-builder-function)\n",
    "5. [Model loading and setup](#Model-loading-and-setup)\n",
    "6. [Data generator configuration](#Data-generator-configuration)\n",
    "7. [Model checkpoint setup](#Model-checkpoint-setup)\n",
    "8. [Model training and compilation](#Model-training-and-compilation)\n",
    "9. [Model shape validation](#Model-shape-validation)\n",
    "10. [Training results visualization](#Training-results-visualization)\n",
    "\n",
    "</font> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f18f62d",
   "metadata": {},
   "source": [
    "## Data download and extraction\n",
    "Let's begin by downloading the dataset to evaluate the models.\n",
    "Here, you declare:\n",
    "1. The dataset URL from which the dataset would be downloaded\n",
    "2. The dataset downloading primary function, based on the `skillsnetwork` library\n",
    "3. The dataset fallback downloading function, based on regular `http` downloading functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a9f0820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write permissions available for downloading and extracting the dataset tar file\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b4135c75d06478eb30e3890a336008b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading images-dataSAT.tar:   0%|          | 0/20243456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c74cebdd39642e5834f05f0bc30fc28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6003 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import skillsnetwork\n",
    "\n",
    "data_dir = \".\"\n",
    "dataset_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/4Z1fwRR295-1O3PMQBH6Dg/images-dataSAT.tar\"\n",
    "\n",
    "\n",
    "def check_skillnetwork_extraction(extract_dir):\n",
    "    \"\"\"Check if the environment allows symlink creation for download/extraction.\"\"\"\n",
    "    symlink_test = os.path.join(extract_dir, \"symlink_test\")\n",
    "    if not os.path.exists(symlink_test):\n",
    "        os.symlink(os.path.join(os.sep, \"tmp\"), symlink_test)\n",
    "        print(\"Write permissions available for downloading and extracting the dataset tar file\")\n",
    "        os.unlink(symlink_test)\n",
    "\n",
    "async def download_tar_dataset(url, tar_path, extract_dir):\n",
    "    \"\"\"Download and extract dataset tar file asynchronously.\"\"\"\n",
    "    if not os.path.exists(tar_path):\n",
    "        try:\n",
    "            print(f\"Downloading from {url}...\")\n",
    "            import httpx\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                response = await client.get(url, follow_redirects=True)\n",
    "                response.raise_for_status()\n",
    "                with open(tar_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "            print(f\"Successfully downloaded '{tar_path}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Download error: {e}\")\n",
    "    else:\n",
    "        print(f\"Dataset tar file already exists at: {tar_path}\")\n",
    "    import tarfile\n",
    "    with tarfile.open(tar_path, 'r:*') as tar_ref:\n",
    "        tar_ref.extractall(path=extract_dir)\n",
    "        print(f\"Successfully extracted to '{extract_dir}'.\")\n",
    "\n",
    "try:\n",
    "    check_skillnetwork_extraction(data_dir)\n",
    "    await skillsnetwork.prepare(url=dataset_url, path=data_dir, overwrite=True)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"Primary download/extraction method failed.\")\n",
    "    print(\"Falling back to manual download and extraction...\")\n",
    "    import tarfile\n",
    "    import httpx\n",
    "    from pathlib import Path\n",
    "    file_name = Path(dataset_url).name\n",
    "    tar_path = os.path.join(data_dir, file_name)\n",
    "    await download_tar_dataset(dataset_url, tar_path, data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c1f18c",
   "metadata": {},
   "source": [
    "## Package installation\n",
    "\n",
    "Install the required basic Python packages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c7ca672",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 41.8 ms, sys: 33.3 ms, total: 75 ms\n",
      "Wall time: 10 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture captured_output\n",
    "%pip install numpy==1.26\n",
    "%pip install matplotlib==3.9.2\n",
    "%pip install skillsnetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd033f37-165f-41b8-90e2-a2439f035575",
   "metadata": {},
   "source": [
    "### Install Tensorflow library for Keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d66b191c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.19\n",
      "  Downloading tensorflow-2.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow==2.19)\n",
      "  Downloading absl_py-2.4.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow==2.19)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow==2.19)\n",
      "  Downloading flatbuffers-25.12.19-py2.py3-none-any.whl.metadata (1.0 kB)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow==2.19)\n",
      "  Downloading gast-0.7.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow==2.19)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow==2.19)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow==2.19)\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (24.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow==2.19)\n",
      "  Downloading protobuf-5.29.6-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow==2.19)\n",
      "  Downloading termcolor-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow==2.19)\n",
      "  Downloading wrapt-2.1.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow==2.19)\n",
      "  Downloading grpcio-1.78.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow==2.19)\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow==2.19)\n",
      "  Downloading keras-3.13.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (1.26.0)\n",
      "Collecting h5py>=3.11.0 (from tensorflow==2.19)\n",
      "  Downloading h5py-3.15.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow==2.19)\n",
      "  Downloading ml_dtypes-0.5.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.9 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow==2.19) (0.45.1)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow==2.19)\n",
      "  Downloading rich-14.3.2-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow==2.19)\n",
      "  Downloading namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow==2.19)\n",
      "  Downloading optree-0.18.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (34 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.19) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.19) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.19) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.19) (2024.12.14)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow==2.19)\n",
      "  Downloading markdown-3.10.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow==2.19)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow==2.19)\n",
      "  Downloading werkzeug-3.1.5-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /opt/conda/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow==2.19) (3.0.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow==2.19)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow==2.19) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.19)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Downloading tensorflow-2.19.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (645.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m645.0/645.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.4.0-py3-none-any.whl (135 kB)\n",
      "Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.12.19-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.7.0-py3-none-any.whl (22 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.78.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (6.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.15.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading keras-3.13.2-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.5/24.5 MB\u001b[0m \u001b[31m39.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading ml_dtypes-0.5.4-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Downloading protobuf-5.29.6-cp38-abi3-manylinux2014_x86_64.whl (320 kB)\n",
      "Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m40.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-3.3.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading wrapt-2.1.1-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (121 kB)\n",
      "Downloading markdown-3.10.2-py3-none-any.whl (108 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading werkzeug-3.1.5-py3-none-any.whl (225 kB)\n",
      "Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.18.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (408 kB)\n",
      "Downloading rich-14.3.2-py3-none-any.whl (309 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorboard-data-server, protobuf, optree, opt-einsum, ml-dtypes, mdurl, markdown, h5py, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, markdown-it-py, rich, keras, tensorflow\n",
      "Successfully installed absl-py-2.4.0 astunparse-1.6.3 flatbuffers-25.12.19 gast-0.7.0 google-pasta-0.2.0 grpcio-1.78.0 h5py-3.15.1 keras-3.13.2 libclang-18.1.1 markdown-3.10.2 markdown-it-py-4.0.0 mdurl-0.1.2 ml-dtypes-0.5.4 namex-0.1.0 opt-einsum-3.4.0 optree-0.18.0 protobuf-5.29.6 rich-14.3.2 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 termcolor-3.3.0 werkzeug-3.1.5 wrapt-2.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 1.6 s, sys: 357 ms, total: 1.96 s\n",
      "Wall time: 1min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install tensorflow==2.19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e461a6d4-57ea-4431-bdc9-c30e436999f0",
   "metadata": {},
   "source": [
    "### Install SkLearn ML library for evaluation metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03c5eb16-6bee-4ca9-bb01-271f0f708743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn==1.7.0\n",
      "  Downloading scikit_learn-1.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn==1.7.0) (1.26.0)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn==1.7.0)\n",
      "  Downloading scipy-1.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn==1.7.0)\n",
      "  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn==1.7.0)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy>=1.22.0 (from scikit-learn==1.7.0)\n",
      "  Downloading numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Downloading scikit_learn-1.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m127.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Downloading scipy-1.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (35.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.0/35.0 MB\u001b[0m \u001b[31m169.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m140.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, numpy, joblib, scipy, scikit-learn\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.0\n",
      "    Uninstalling numpy-1.26.0:\n",
      "      Successfully uninstalled numpy-1.26.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed joblib-1.5.3 numpy-2.4.2 scikit-learn-1.7.0 scipy-1.17.0 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 309 ms, sys: 78.5 ms, total: 388 ms\n",
      "Wall time: 19.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install scikit-learn==1.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134c4ade",
   "metadata": {},
   "source": [
    "## Library imports and setup\n",
    "\n",
    "Import essential libraries for data manipulation and visualization, and suppress warnings for cleaner notebook output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd0fcdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 765 ms, sys: 73.9 ms, total: 839 ms\n",
      "Wall time: 925 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import httpx\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "def present_time():\n",
    "        return datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a0a554",
   "metadata": {},
   "source": [
    "### TensorFlow/Keras library imports\n",
    "\n",
    "Sets environment variables to reduce TensorFlow logging noise and imports Keras modules for model building and training. Detects GPU availability for device assignment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "99804321",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1771366137.947054     376 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1771366137.953164     376 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1771366137.969401     376 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771366137.969423     376 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771366137.969425     376 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771366137.969427     376 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available for training: cpu\n",
      "CPU times: user 2.76 s, sys: 417 ms, total: 3.18 s\n",
      "Wall time: 3.23 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.initializers import HeUniform\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "gpu_list = tf.config.list_physical_devices('GPU')\n",
    "device = \"gpu\" if gpu_list != [] else \"cpu\"\n",
    "print(f\"Device available for training: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc53fe7",
   "metadata": {},
   "source": [
    "## Model download helper\n",
    "\n",
    "Now, define an asynchronous function to download model files from given URLs, if they are not already present locally. \n",
    "You use `httpx` for asynchronous HTTP requests with error handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e211b54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def download_model(url, model_path):\n",
    "    if not os.path.exists(model_path):\n",
    "        try:\n",
    "            print(f\"Downloading from {url}...\")\n",
    "            import httpx\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                response = await client.get(url, follow_redirects=True)\n",
    "                response.raise_for_status()\n",
    "                with open(model_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "            print(f\"Successfully downloaded '{model_path}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Download error: {e}\")\n",
    "    else:\n",
    "        print(f\"Model file already downloaded at: {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e570be57-93e3-49cd-9832-4979909cbeab",
   "metadata": {},
   "source": [
    "## Lab layout\n",
    "1. You will start by loading a pre-trained Convolutional Neural Network (CNN) to act as a robust feature extractor for your image dataset.\n",
    "2. After loading your CNN, you’ll select an intermediate feature map and reshape it into a sequence of tokens, getting your data ready for transformer-based learning.\n",
    "3. You’ll add custom positional embeddings to your tokens so that the model can retain the original spatial structure of your images, even after the features have been flattened.\n",
    "4. Next, you implement a Vision Transformer (ViT) encoder by stacking several transformer blocks, allowing your model to learn global relationships and context throughout the image.\n",
    "5. You’ll combine the CNN and ViT encoder into a single, hybrid model so that you can leverage both the local feature extraction power of CNNs and the global attention mechanism of transformers.\n",
    "6. When preparing your dataset, you’ll use Keras’s ImageDataGenerator to handle data augmentation and to properly encode your labels for multi-class image classification.\n",
    "7. You’ll set up a model checkpoint callback, letting your model automatically save its best weights whenever validation accuracy improves during training, so you always keep the most effective model.\n",
    "8. To ensure everything works smoothly, you’ll check the input and output shapes, which helps you catch architectural mistakes early.\n",
    "9. As your model trains, you’ll visualize both training and validation accuracy and loss, which will help you monitor performance and spot signs of overfitting or underfitting.\n",
    "10. Throughout the process, you can follow clear explanations in the notebook, making it easy for you to understand how each component—from CNN to transformer—is integrated to achieve stronger image classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e94bd9",
   "metadata": {},
   "source": [
    "## Model paths and download\n",
    "\n",
    "In the cell below, you define the file paths and URLs for the Keras and PyTorch models and download them using the `download_model` function defined above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be04a5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file already downloaded at: ./ai-capstone-keras-best-model-model_downloaded.keras\n"
     ]
    }
   ],
   "source": [
    "data_dir = \".\"\n",
    "\n",
    "keras_model_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/U-uPeyCyOQYh0GrZPGsqoQ/ai-capstone-keras-best-model-model.keras\"\n",
    "keras_model_name = \"ai-capstone-keras-best-model-model_downloaded.keras\"\n",
    "keras_model_path = os.path.join(data_dir, keras_model_name)\n",
    "\n",
    "await download_model(keras_model_url, keras_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c801ed-dedc-4148-9c1f-6dd432d91093",
   "metadata": {},
   "source": [
    "### Reproducibility with random seeds\n",
    "\n",
    "Here we fix the random seeds for `random` module, NumPy, and TensorFlow. By initializing these seeds with a constant value (for example, 42), any operations that involve randomness (such as weight initialization, data shuffling, or data augmentation) will produce the exact same sequence of random numbers every time the code is run. This is crucial for ensuring the reproducibility of experimental results and when comparing different models or hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c483241b-9792-44e9-967f-2df9a6ff2b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "seed_value = 7331\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2b3d89",
   "metadata": {},
   "source": [
    "## Model loading and setup\n",
    "\n",
    "Here, you will load a pre-trained CNN model and learn to work with saved Keras models and prepare them for use in the hybrid architecture.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124a12a8-aafb-48ec-bd8a-bb90073fd971",
   "metadata": {},
   "source": [
    "## Task 1: Load the pre-trained CNN model in `cnn_model` variable using `load_model()` function and print model summary using `summary()` method\n",
    "\n",
    "The `load_model()` function loads the complete Keras model, including architecture, weights, and compilation state. The loaded model serves as the CNN backbone for feature extraction in the hybrid architecture. The `cnn_model.summary()` line can be uncommented to inspect the model architecture and identify appropriate layers for feature extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34f9f5e1-b775-45e1-8bf7-ee0856b5a4c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,432</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">51,264</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">204,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">819,456</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,277,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">13,108,224</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,099,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,049</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m2,432\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m51,264\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │       \u001b[38;5;34m204,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │       \u001b[38;5;34m819,456\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │     \u001b[38;5;34m3,277,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │    \u001b[38;5;34m13,108,224\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │         \u001b[38;5;34m4,096\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m65,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m8,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m131,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │       \u001b[38;5;34m525,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │         \u001b[38;5;34m4,096\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │     \u001b[38;5;34m2,099,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │         \u001b[38;5;34m8,192\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │         \u001b[38;5;34m2,049\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">61,034,501</span> (232.83 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m61,034,501\u001b[0m (232.83 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,340,801</span> (77.59 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m20,340,801\u001b[0m (77.59 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,096</span> (47.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m12,096\u001b[0m (47.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">40,681,604</span> (155.19 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m40,681,604\u001b[0m (155.19 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Please use the space below to write your answer\n",
    "cnn_model = load_model(keras_model_path)\n",
    "cnn_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2ccbcff-9fd5-4bb3-bc6a-cf00e3339009",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "cnn_model = load_model(keras_model_path) # Loading the CNN model\n",
    "\n",
    "cnn_model.summary() # Display model summary\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc551ed-bce9-4b1e-b615-ff5c608df8fa",
   "metadata": {},
   "source": [
    "## Task 2: Based on `model.summary()`, get the name of the layer from the CNN model for feature extraction in the variable `feature_layer_name`\n",
    "\n",
    "This is the last convolutional layer, usually before `GlobalAveragePooling2D`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88a6ab36-c62a-4f2e-906f-761282cd788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Please use the space below to write your answer\n",
    "\n",
    "feature_layer_name = \"batch_normalization_5\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbb3c91-32d8-4f89-9be1-9a5b3997b3e4",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "feature_layer_name = \"batch_normalization_5\"\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbea4d3",
   "metadata": {},
   "source": [
    "## Custom positional embedding layer\n",
    "\n",
    "In this cell, you define a custom Keras layer called `AddPositionEmbedding` that implements positional embeddings for the Vision Transformer architecture. Positional embeddings are crucial in transformer models because they lack inherent spatial awareness, unlike convolutional layers that have built-in spatial inductive biases.\n",
    "\n",
    "- The class inherits from `layers.Layer`, making it a proper Keras custom layer\n",
    "- In the `__init__` method, it creates a trainable weight matrix using `self.add_weight()`\n",
    "- The positional embedding has shape `(1, num_patches, embed_dim)` where the first dimension allows broadcasting across batch sizes\n",
    "- The `initializer=\"random_normal\"` ensures the embeddings start with random values that will be learned during training\n",
    "- The `trainable=True` parameter makes these embeddings learnable parameters\n",
    "\n",
    "\n",
    "This layer is essential for the hybrid CNN-ViT architecture because when CNN feature maps are flattened into tokens, spatial relationships are lost. The positional embeddings restore spatial awareness by providing each token with information about its original spatial location in the feature map. This allows the transformer to understand which tokens are spatially adjacent or distant, enabling it to make spatially aware attention decisions.\n",
    "\n",
    "The `call` method adds the positional embeddings to the input tokens using element-wise addition. This is computationally efficient and follows the standard transformer approach, where positional information is added to preserve the embedding dimension.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bf233bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Positional embedding that Keras can track\n",
    "@tf.keras.utils.register_keras_serializable(package=\"Custom\")\n",
    "class AddPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, num_patches, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_patches = num_patches\n",
    "        self.embed_dim   = embed_dim\n",
    "        self.pos = self.add_weight(\n",
    "            name=\"pos_embedding\",\n",
    "            shape=(1, num_patches, embed_dim),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True)\n",
    "\n",
    "    def call(self, tokens):\n",
    "        return tokens + self.pos\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"num_patches\": self.num_patches,\n",
    "            \"embed_dim\":   self.embed_dim,\n",
    "        })\n",
    "        return {**config}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8388357",
   "metadata": {},
   "source": [
    "## Transformer block implementation\n",
    "\n",
    "In this code cell, you will implement a complete transformer encoder block, the fundamental building block of the Vision Transformer architecture. The `TransformerBlock` class encapsulates the standard transformer encoder architecture with multi-head self-attention and feed-forward neural network components.\n",
    "\n",
    "**Role in hybrid architecture:**\n",
    "In the CNN-ViT hybrid, these transformer blocks process the tokenized CNN feature maps, allowing the model to capture long-range spatial dependencies that CNNs might miss due to their limited receptive fields. The self-attention mechanism enables each spatial location to attend to all other locations, providing global context awareness.\n",
    "\n",
    "**Technical architecture:**\n",
    "- **Multi-head attention (MHA):** Uses `layers.MultiHeadAttention` with a configurable number of heads and key dimension equal to embed_dim\n",
    "- **Layer normalization:** Two `LayerNormalization` layers with epsilon=1e-6 for numerical stability\n",
    "- **MLP block:** A two-layer feed-forward network with GELU activation and dropout for regularization\n",
    "- **Residual connections:** Implements skip connections around both the attention and MLP blocks\n",
    "\n",
    "**Parameters:**\n",
    "- `embed_dim`: The dimensionality of token embeddings (typically matches CNN feature map channels)\n",
    "- `num_heads`: Number of attention heads (default 8, must divide embed_dim evenly)\n",
    "- `mlp_dim`: Hidden dimension of the MLP block (typically 4x embed_dim)\n",
    "- `dropout`: Dropout rate for regularization (default 0.1)\n",
    "\n",
    "**Forward pass logic:**\n",
    "Forward pass allows the model to capture both local and global dependencies in the feature representations while maintaining gradient flow through residual connections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "727fb05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Transformer encoder block\n",
    "@tf.keras.utils.register_keras_serializable(package=\"Custom\")\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads=8, mlp_dim=2048, dropout=0.1, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.mlp_dim   = mlp_dim\n",
    "        self.dropout   = dropout\n",
    "        self.mha  = layers.MultiHeadAttention(num_heads, key_dim=embed_dim)\n",
    "        self.norm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.mlp = tf.keras.Sequential([\n",
    "            layers.Dense(mlp_dim, activation=\"gelu\"),\n",
    "            layers.Dropout(dropout),\n",
    "            layers.Dense(embed_dim),\n",
    "            layers.Dropout(dropout)\n",
    "        ])\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.norm1(x + self.mha(x, x))\n",
    "        return self.norm2(x + self.mlp(x))\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\":  self.embed_dim,\n",
    "            \"num_heads\":  self.num_heads,\n",
    "            \"mlp_dim\":    self.mlp_dim,\n",
    "            \"dropout\":    self.dropout,\n",
    "        })\n",
    "        return {**config}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ff6478",
   "metadata": {},
   "source": [
    "## Hybrid model builder function\n",
    "\n",
    "Now, you will define a function `build_cnn_vit_hybrid` that constructs the complete hybrid CNN-Vision Transformer model. This function represents the main architectural innovation of the notebook, combining the strengths of convolutional neural networks for local feature extraction with transformers for global context modeling.\n",
    "\n",
    "**Function architecture:**\n",
    "1. **CNN feature extraction:** Extracts intermediate feature maps from a pre-trained CNN at a specified layer\n",
    "2. **Tokenization:** Reshapes spatial feature maps into a sequence of tokens suitable for transformer processing\n",
    "3. **Positional encoding:** Adds learnable positional embeddings to maintain spatial relationships\n",
    "4. **Transformer stack:** Applies multiple transformer encoder blocks for global context modeling\n",
    "5. **Classification head:** Pools tokens and applies the final classification layer\n",
    "\n",
    "**Parameters:**\n",
    "- `cnn_model`: Pre-trained CNN model for feature extraction\n",
    "- `feature_layer_name`: Name of the CNN layer to extract features from (e.g., `batch_normalization_5` in the original model architecture)\n",
    "- `num_transformer_layers`: Number of transformer blocks to stack (default 4)\n",
    "- `num_heads`: Number of attention heads per transformer block (default 8)\n",
    "- `mlp_dim`: MLP hidden dimension in transformer blocks (default 2048)\n",
    "- `num_classes`: Number of output classes for classification\n",
    "\n",
    "The function first freezes the CNN backbone (`cnn_model.trainable = False`) to use it as a fixed feature extractor. It then extracts feature maps with shape (B, H, W, C) and reshapes them to (B, H*W, C), where each spatial location becomes a token. The `AddPositionEmbedding` layer adds spatial awareness, and multiple TransformerBlock layers process the tokens. Finally, `GlobalAveragePooling1D` aggregates all tokens, and a dense layer with softmax activation produces class predictions.\n",
    "\n",
    "This hybrid approach leverages CNN's local feature detection capabilities while adding the transformer's global attention mechanism. The result is a model that can capture both fine-grained local patterns and long-range spatial dependencies, potentially **outperforming pure CNN** or pure transformer approaches on vision tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4c2598f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn_vit_hybrid(\n",
    "        cnn_model,\n",
    "        feature_layer_name,\n",
    "        num_transformer_layers=4,\n",
    "        num_heads=8,\n",
    "        mlp_dim=2048,\n",
    "        num_classes=2):\n",
    "    # 1. Freeze or fine-tune the CNN as you prefer\n",
    "    cnn_model.trainable = False      # set True to fine-tune\n",
    "    \n",
    "    # 2. Feature extractor up to the chosen layer\n",
    "    features = cnn_model.get_layer(feature_layer_name).output\n",
    "    H, W, C = features.shape[1], features.shape[2], features.shape[3]\n",
    "    \n",
    "    # 3. Flatten spatial grid → tokens  &  add positional encoding\n",
    "    x = layers.Reshape((H * W, C))(features) \n",
    "    x = AddPositionEmbedding(H * W, C)(x)\n",
    "\n",
    "    # 4. Stack ViT encoder blocks\n",
    "    for _ in range(num_transformer_layers):\n",
    "        x = TransformerBlock(C, num_heads, mlp_dim)(x)\n",
    "\n",
    "    # 5. Token pooling & classification head\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "    return Model(cnn_model.layers[0].input, outputs, name=\"CNN_ViT_hybrid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46c3ff0",
   "metadata": {},
   "source": [
    "## Data generator configuration\n",
    "\n",
    "Now you will set up data preprocessing and augmentation pipeline using Keras' `ImageDataGenerator`.\n",
    "\n",
    "**Data configuration parameters:**\n",
    "- `img_w, img_h = 64, 64`: Input image dimensions (64x64 pixels)\n",
    "- `n_channels = 3`: RGB color channels\n",
    "- `batch_size = 128`: Number of samples per training batch\n",
    "- `num_classes = 2`: Binary classification setup\n",
    "\n",
    "**Generators:**\n",
    "Two separate generators are created:\n",
    "1. `train_gen`: Training data with augmentation and shuffling\n",
    "2. `val_gen`: Validation data with the same preprocessing but a different subset\n",
    "\n",
    "Both generators use `class_mode=\"categorical\"` for one-hot encoded labels, `target_size=(64,64)` for consistent input dimensions, and `shuffle=True` for randomized batch sampling.\n",
    "\n",
    "This augmentation strategy significantly increases the effective dataset size and helps prevent overfitting by exposing the model to varied versions of the same images. The validation split ensures proper model evaluation on unseen data, while the categorical class mode prepares labels for softmax classification in the hybrid model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5cc8c443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./images_dataSAT\n",
      "Found 4800 images belonging to 2 classes.\n",
      "Found 1200 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset_path = os.path.join(data_dir, \"images_dataSAT\")\n",
    "print(dataset_path)\n",
    "\n",
    "img_w, img_h = 64, 64\n",
    "n_channels = 3\n",
    "batch_size = 4\n",
    "num_classes = 2\n",
    "\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255,\n",
    "                             rotation_range=40, \n",
    "                             width_shift_range=0.2,\n",
    "                             height_shift_range=0.2,\n",
    "                             shear_range=0.2,\n",
    "                             zoom_range=0.2,\n",
    "                             horizontal_flip=True,\n",
    "                             fill_mode=\"nearest\",\n",
    "                             validation_split=0.2\n",
    "                            )\n",
    "\n",
    "train_gen = datagen.flow_from_directory(dataset_path,\n",
    "                                        target_size = (img_w, img_h),\n",
    "                                        batch_size= batch_size,\n",
    "                                        class_mode=\"categorical\",\n",
    "                                        subset=\"training\",\n",
    "                                        shuffle=True\n",
    "                                       )\n",
    "\n",
    "val_gen = datagen.flow_from_directory(dataset_path,\n",
    "                                      target_size =(img_w, img_h),\n",
    "                                      batch_size = batch_size, \n",
    "                                      class_mode=\"categorical\",\n",
    "                                      subset=\"validation\",\n",
    "                                      shuffle=True\n",
    "                                     )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b20ce4",
   "metadata": {},
   "source": [
    "## Model checkpoint setup\n",
    "\n",
    "This code cell configures a `ModelCheckpoint` callback for saving the best model weights during training. Model checkpointing is a crucial practice in deep learning that prevents loss of training progress and enables recovery of the best-performing model state.\n",
    "\n",
    "**Checkpoint configuration:**\n",
    "- `filepath`: Specifies the file path and name for saving weights\n",
    "- `save_weights_only=True`: Saves only model weights, not the full model architecture (more efficient and avoids serialization issues)\n",
    "- `monitor='val_accuracy'`: Tracks validation accuracy as the metric for determining the \"best\" model\n",
    "- `mode='max'`: Indicates that higher validation accuracy values are better (use 'min' for loss metrics)\n",
    "- `save_best_only=True`: Only saves the model when validation accuracy improves, preventing storage of worse-performing checkpoints\n",
    "- `verbose=1`: Provides console output when a checkpoint is saved\n",
    "\n",
    "The checkpoint callback addresses several important training considerations:\n",
    "1. **Overfitting prevention:** Captures the model state at peak validation performance before overfitting occurs\n",
    "2. **Storage efficiency:** Saving weights only reduces file size compared to full model serialization\n",
    "3. **Automatic model saving:** Eliminates manual monitoring by automatically saving the best-performing epoch\n",
    "\n",
    "**Integration with training:**\n",
    "This callback will be passed to the `model.fit()` method, where it will monitor validation accuracy after each epoch. When validation accuracy improves, the callback saves the current model weights to the specified file. This ensures that even if training continues past the optimal point, the best-performing weights are preserved.\n",
    "\n",
    "**File naming convention:**\n",
    "The filename uses the `.model.keras` extension to indicate it contains the full model architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e30bfce4-40cf-4cbc-87ca-a3ffe0c176e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPrintCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        # Print epoch number and starting time\n",
    "        print(f\"Epoch {(epoch + 1):02d} completed on {present_time()}\")\n",
    "time_print_callback = CustomPrintCallback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5a6d3120",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"keras_cnn_vit.model.keras\"\n",
    "# Save only weights to overcome the serialization issues with the hybrid model. The full model can be saved using the model architecture and weights.\n",
    "checkpoint_cb = ModelCheckpoint(filepath=model_name,\n",
    "                                save_weights_only=False,  # Set to True to save only weights\n",
    "                                monitor='val_loss',      # or 'val_accuracy', 'val_loss'\n",
    "                                mode='min',              # 'min' for loss, 'max' for accuracy\n",
    "                                save_best_only=True,\n",
    "                                verbose=1\n",
    "                                \n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d310c72",
   "metadata": {},
   "source": [
    "## Model training and compilation\n",
    "\n",
    "Now, you will set up the core training pipeline, where the hybrid CNN-ViT model is built, compiled, and trained. This is the complete workflow from model instantiation to training execution with proper configuration for multi-class classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2596da50-d549-4f2a-a4b1-152291141de4",
   "metadata": {},
   "source": [
    "## Task 3: Define the model architecture in a variable named `hybrid_model` using the `build_cnn_vit_hybrid` function\n",
    "You may use the following parameters:\n",
    "\n",
    "- feature_layer_name: feature_layer_name\n",
    "- num_transformer_layers: 4\n",
    "- attention heads: 8\n",
    "- mlp dimension: 2048\n",
    "- num_classes: extract from training data generator (train_gen.num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f5abd941-a279-43bf-b4e7-0a89ffc922a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Please use the space below to write your answer\n",
    "\n",
    "num_classes = train_gen.num_classes\n",
    "\n",
    "hybrid_model = build_cnn_vit_hybrid(\n",
    "        cnn_model,\n",
    "        feature_layer_name=feature_layer_name,\n",
    "        num_transformer_layers=4,\n",
    "        num_heads=8,\n",
    "        mlp_dim=2048,\n",
    "        num_classes=train_gen.num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe987db-3731-4866-87d4-69c440feffca",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "num_classes = train_gen.num_classes\n",
    "hybrid_model = build_cnn_vit_hybrid(\n",
    "        cnn_model,\n",
    "        feature_layer_name=feature_layer_name,\n",
    "        num_transformer_layers=4,\n",
    "        num_heads=8,\n",
    "        mlp_dim=2048,\n",
    "        num_classes=train_gen.num_classes)\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47f907b-ec3b-49b8-ae94-c4944bad334d",
   "metadata": {},
   "source": [
    "## Task 4: Compile the model `hybrid_model` \n",
    "\n",
    "You may use the following parameters:\n",
    "- `optimizer=tf.keras.optimizers.Adam`\n",
    "- `learning rate: 0.0001`\n",
    "- `loss: categorical_crossentropy`\n",
    "- `metrics: accuracy`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "43445bc3-e606-4efd-9875-59d8578d69f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Please use the space below to write your answer\n",
    "\n",
    "\n",
    "hybrid_model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "                     loss=\"categorical_crossentropy\",\n",
    "                     metrics=[\"accuracy\"],\n",
    "                    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191748d2-897b-44c8-993e-e5f1697d98f3",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "hybrid_model.compile(optimizer=tf.keras.optimizers.Adam(1e-4),\n",
    "                     loss=\"categorical_crossentropy\",\n",
    "                     metrics=[\"accuracy\"],\n",
    "                    )\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4c5d3f",
   "metadata": {},
   "source": [
    "## Model shape validation\n",
    "\n",
    "This code cell performs the validation step to ensure the hybrid model produces outputs with the correct shape and dimensions. Shape validation is essential in deep learning to catch architectural errors early and verify that the model will work correctly with the expected input and output formats.\n",
    "\n",
    "**Validation process:**\n",
    "The cell creates a dummy input tensor using `tf.random.normal([1, img_w, img_h, n_channels])`, which generates random values with the same shape as actual input images:\n",
    "- Batch size: 1 (single sample for testing)\n",
    "- Width: `img_w` (64 pixels)\n",
    "- Height: `img_h` (64 pixels)\n",
    "- Channels: `n_channels` (3 for RGB)\n",
    "\n",
    "**Output verification:**\n",
    "The dummy input is passed through the hybrid model (`hybrid_model(dummy)`) to generate predictions. The expected output shape should be `(1, num_classes)` where:\n",
    "- First dimension (1): Batch size\n",
    "- Second dimension (`num_classes`): Number of classification classes\n",
    "\n",
    "**Technical benefits:**\n",
    "This validation step serves multiple purposes:\n",
    "1. **Architecture verification:** Confirms that all layers are properly connected and compatible\n",
    "2. **Dimension checking:** Ensures the model produces the expected output shape for classification\n",
    "3. **Early error detection:** Catches shape mismatches before actual training or inference\n",
    "4. **Model readiness:** Verifies the model is ready for production use\n",
    "\n",
    "**Importance:**\n",
    "If the output shape doesn't match expectations, it indicates potential issues in the hybrid architecture, such as incorrect reshaping operations, wrong number of classes configuration, or problems in the CNN-to-transformer transition. This simple test can save significant debugging time by catching architectural issues immediately after model construction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22cbcd1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: (1, 2)\n"
     ]
    }
   ],
   "source": [
    "# Ensure end-to-end shapes line up\n",
    "dummy = tf.random.normal([1, img_w, img_h, n_channels])\n",
    "pred  = hybrid_model(dummy)\n",
    "print(\"Logits shape:\", pred.shape)   # should be (1, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1187a326-5472-4ade-b14d-3baf9992ae8a",
   "metadata": {},
   "source": [
    "## Task 5: Define the training configuration of the `hybrid_model`.\n",
    "In the interest of time, you can train for 3 epochs.\n",
    "Use the `checkpoint_cb` callback keyword for automatic saving of the best model state. \n",
    "\n",
    "To make sure that the computational resources are not overloaded, we will limit the number of batches used for training in each epoch. This can be done by **`steps_per_epoch`**. \n",
    "\n",
    "For this task use  **`steps_per_epoch = 128`**\n",
    "\n",
    "Feel free to play with these parameters if you are executing this on your local machine or any other platform.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ccf54504-7295-4db0-bd24-654b05a931c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8s/step - accuracy: 0.9861 - loss: 0.3370   \n",
      "Epoch 1: val_loss improved from None to 0.07764, saving model to keras_cnn_vit.model.keras\n",
      "\n",
      "Epoch 1: finished saving model to keras_cnn_vit.model.keras\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1166s\u001b[0m 9s/step - accuracy: 0.9883 - loss: 0.2100 - val_accuracy: 0.9892 - val_loss: 0.0776\n",
      "Epoch 2/3\n",
      "\u001b[1m110/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m2:34\u001b[0m 9s/step - accuracy: 0.9880 - loss: 0.1896 "
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nFileNotFoundError: [Errno 2] No such file or directory: './images_dataSAT/class_0_non_agri/tile_S2A_MSIL2A_20250427T101701_N0511_R065_T32UQB_20250427T170513.SAFE_21049.jpg'\nTraceback (most recent call last):\n\n  File \"/opt/conda/lib/python3.12/site-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.12/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py\", line 264, in _finite_generator\n    yield self._standardize_batch(self.py_dataset[i])\n                                  ~~~~~~~~~~~~~~~^^^\n\n  File \"/opt/conda/lib/python3.12/site-packages/keras/src/legacy/preprocessing/image.py\", line 71, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.12/site-packages/keras/src/legacy/preprocessing/image.py\", line 316, in _get_batches_of_transformed_samples\n    img = image_utils.load_img(\n          ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.12/site-packages/keras/src/utils/image_utils.py\", line 247, in load_img\n    with open(path, \"rb\") as f:\n         ^^^^^^^^^^^^^^^^\n\nFileNotFoundError: [Errno 2] No such file or directory: './images_dataSAT/class_0_non_agri/tile_S2A_MSIL2A_20250427T101701_N0511_R065_T32UQB_20250427T170513.SAFE_21049.jpg'\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_multi_step_on_iterator_18720]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 13\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError: The file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;66;03m# Proceed with loading the image\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     fit \u001b[38;5;241m=\u001b[39m \u001b[43mhybrid_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_cb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m                       \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\n\u001b[1;32m     18\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m)\u001b[49m \n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nFileNotFoundError: [Errno 2] No such file or directory: './images_dataSAT/class_0_non_agri/tile_S2A_MSIL2A_20250427T101701_N0511_R065_T32UQB_20250427T170513.SAFE_21049.jpg'\nTraceback (most recent call last):\n\n  File \"/opt/conda/lib/python3.12/site-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.12/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py\", line 264, in _finite_generator\n    yield self._standardize_batch(self.py_dataset[i])\n                                  ~~~~~~~~~~~~~~~^^^\n\n  File \"/opt/conda/lib/python3.12/site-packages/keras/src/legacy/preprocessing/image.py\", line 71, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.12/site-packages/keras/src/legacy/preprocessing/image.py\", line 316, in _get_batches_of_transformed_samples\n    img = image_utils.load_img(\n          ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.12/site-packages/keras/src/utils/image_utils.py\", line 247, in load_img\n    with open(path, \"rb\") as f:\n         ^^^^^^^^^^^^^^^^\n\nFileNotFoundError: [Errno 2] No such file or directory: './images_dataSAT/class_0_non_agri/tile_S2A_MSIL2A_20250427T101701_N0511_R065_T32UQB_20250427T170513.SAFE_21049.jpg'\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_multi_step_on_iterator_18720]"
     ]
    }
   ],
   "source": [
    "## Please use the space below to write your answer\n",
    "\n",
    "import os\n",
    "\n",
    "file_path = './images_dataSAT/class_1_agri/tile_S2A_MSIL2A_20250427T101701_N0511_R065_T32UQB_20250427T170513.SAFE_8416.jpg'\n",
    "\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"Error: The file {file_path} does not exist.\")\n",
    "else:\n",
    "    # Proceed with loading the image\n",
    "\n",
    "\n",
    "    fit = hybrid_model.fit(train_gen,\n",
    "                       epochs=3,\n",
    "                       validation_data=val_gen,\n",
    "                       callbacks=[checkpoint_cb],\n",
    "                       steps_per_epoch = 128\n",
    "                        ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81bcb41b-5739-47e9-817d-b8ea7a32b7ff",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "\n",
    "fit = hybrid_model.fit(train_gen,\n",
    "                       epochs=3,\n",
    "                       validation_data=val_gen,\n",
    "                       callbacks=[checkpoint_cb],\n",
    "                       steps_per_epoch = 128\n",
    "                        )\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e06e06",
   "metadata": {},
   "source": [
    "## Training results visualization\n",
    "\n",
    "This code cell creates comprehensive visualizations of the training process by plotting both accuracy and loss curves. Visualization of training metrics is essential for understanding model performance, diagnosing training issues, and making informed decisions about model optimization.\n",
    "\n",
    "**Visualization setup:**\n",
    "The cell uses matplotlib to create two separate plots with consistent styling:\n",
    "- `fig_w, fig_h`: Sets figure dimensions for compact, readable plots\n",
    "- `plt.subplots(figsize=(fig_w, fig_h))`: Creates a subplot with specified dimensions\n",
    "\n",
    "**Accuracy plot analysis:**\n",
    "The first plot displays training and validation accuracy over epochs:\n",
    "- `fit.history['accuracy']`: Training accuracy progression\n",
    "- `fit.history['val_accuracy']`: Validation accuracy progression\n",
    "\n",
    "**Loss plot analysis:**\n",
    "The second plot shows training and validation loss curves:\n",
    "- `fit.history['loss']`: Training loss progression\n",
    "- `fit.history['val_loss']`: Validation loss progression\n",
    "\n",
    "***Importance:***\n",
    "These plots enable several important analyses:\n",
    "1. **Overfitting detection:** Diverging training and validation curves indicate overfitting\n",
    "2. **Convergence assessment:** Plateauing curves suggest training completion\n",
    "3. **Learning rate evaluation:** Oscillating curves may indicate learning rate issues\n",
    "4. **Model performance:** Final accuracy and loss values indicate overall model quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25c26fab",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(figsize\u001b[38;5;241m=\u001b[39m(fig_w, fig_h ))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Plot Accuracy on the first subplot\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m axs\u001b[38;5;241m.\u001b[39mplot(\u001b[43mfit\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m axs\u001b[38;5;241m.\u001b[39mplot(fit\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m axs\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fit' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a figure with a subplot\n",
    "fig_w, fig_h = 4,3\n",
    "fig, axs = plt.subplots(figsize=(fig_w, fig_h ))\n",
    "\n",
    "# Plot Accuracy on the first subplot\n",
    "axs.plot(fit.history['accuracy'], label='Training Accuracy')\n",
    "axs.plot(fit.history['val_accuracy'], label='Validation Accuracy')\n",
    "axs.set_title('Model Accuracy')\n",
    "axs.set_xlabel('Epochs')\n",
    "axs.set_ylabel('Accuracy')\n",
    "axs.legend()\n",
    "axs.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "## You can use this cell to type the code to complete the task.\n",
    "\n",
    "fig, axs = plt.subplots(figsize=(fig_w, fig_h ))\n",
    "\n",
    "# Plot Loss on the second subplot\n",
    "axs.plot(fit.history['loss'], label='Training Loss')\n",
    "axs.plot(fit.history['val_loss'], label='Validation Loss')\n",
    "axs.set_title('Model Loss')\n",
    "axs.set_xlabel('Epochs')\n",
    "axs.set_ylabel('Loss')\n",
    "axs.legend()\n",
    "axs.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9a580f2-7f09-4392-a807-b53419d4c8b1",
   "metadata": {},
   "source": [
    "## Save and download the trained model weights\n",
    "\n",
    "You have successfully trained the ViT model for classification of agricultural land from satellite imagery using **Keras**\n",
    "In this lab, in the interest of time, you have trained the model for 3-5 epochs. However, usually you need to train the model for around 15-20 epochs, depending on the quality of training data and model metrics based on validation. \n",
    "\n",
    "For your convenience, I have saved a model state dict for the model trained over 20 epochs **[here](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/7uNMQhNyTA8qSSDGn5Cc7A/keras-cnn-vit-ai-capstone.keras)**. You can download that for evaluation and further labs on your local machine from **[this link](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/7uNMQhNyTA8qSSDGn5Cc7A/keras-cnn-vit-ai-capstone.keras)**.\n",
    "\n",
    "\n",
    "Otherwise, you have also saved the model state dictionary for the best model using the `checkpoint_cb` callback function during training in this lab.\n",
    "\n",
    "You can also download the model state dict for the model that you have just trained for use in the subsequent labs.\n",
    "\n",
    "This is the PyTorch AI model state that can now be used for infering un-classified images. \n",
    "\n",
    "- You can download the trained model weights: `keras_cnn_vit.model.keras` from the left pane and save it on your local computer. \n",
    "- You can download this model by \"right-click\" on the file and then Clicking \"Download\".\n",
    "- In conjunction with the model architecture, these model weights can be used in other labs of this AI capstone course, instead of the weights provided at the above link\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02fe9d4e-2621-49cd-9118-df88597f36ce",
   "metadata": {},
   "source": [
    "## Save and download the notebook for **final project** submission and evaluation\n",
    "\n",
    "You will need to save and download the completed notebook for final project submission and evaluation. \n",
    "<br>For saving and downloading the completed notebook, please follow the steps given below:</br>\n",
    "\n",
    "<font size = 4>  \n",
    "\n",
    "1) **Complete** all the tasks and questions given in the notebook.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/nv4jHlPU5_R1q7ZJrZ69eg/DL0321EN-M1L1-Save-IPYNB-Screenshot-1.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "2) **Save** the notebook.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/9-WPWD4mW1d-RV5Il5otTg/DL0321EN-M1L1-Save-IPYNB-Screenshot-2.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "3) Identify and right click on the **correct notebook file** in the left pane.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/RUSRPw7NT6Sof94B7-9naQ/DL0321EN-M1L1-Save-IPYNB-Screenshot-3.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "4) Click on **Download**.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/HHry4GT-vhLEcRi1T_LHGg/DL0321EN-M1L1-Save-IPYNB-Screenshot-4.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "5) Download and **Save** the Jupyter notebook file on your computer **for final submission**.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/hhsJbxc6R-T8_pXQGjMjvg/DL0321EN-M1L1-Save-IPYNB-Screenshot-5.png\" style=\"width:600px; border:0px solid black;\">\n",
    "  </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c4ff47-d95c-400f-ae87-56ca79b2ba98",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Congratulations! You've successfully built a CNN-ViT hybrid image classification model.\n",
    "In this lab, you learnt how to combine a convolutional neural network (CNN) with a Vision Transformer (ViT) for advanced image classification tasks. Starting from a pre-trained CNN, you learnt how to extract intermediate features, reshape them as tokens, and provide them with positional embeddings. By stacking transformer encoder blocks on top, the model benefits from both local detail extraction and global context awareness. Throughout the lab, techniques for robust data preparation, efficient training with model checkpoints, and effective visualization of performance were covered. By completing the steps in this notebook, you now have hands-on experience implementing and evaluating a contemporary hybrid vision model using Keras.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca5057e-a8f6-478d-8639-fd70fee4f8eb",
   "metadata": {},
   "source": [
    "<h2>Author</h2>\n",
    "\n",
    "[Aman Aggarwal](https://www.linkedin.com/in/aggarwal-aman)\n",
    "\n",
    "Aman Aggarwal is a PhD working at the intersection of neuroscience, AI, and drug discovery. He specializes in quantitative microscopy and image processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e075dc2f-6ffa-45a6-b2d8-860217305244",
   "metadata": {},
   "source": [
    "<!--\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2025-07-14  | 1.0  | Aman  |  Created the lab |\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917371aa-f1b6-469e-b57f-cbb963d3eef7",
   "metadata": {},
   "source": [
    "© Copyright IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "61f2830ce2097e8820ce6407e6567640ed4feb54a55a17665fb7f3f613234ff1"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
