{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dff419f-5632-47ba-b342-de1e7a9536e1",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center;\">\n",
    "  <a href=\"https://cognitiveclass.ai/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkDL0321ENSkillsNetwork951-2022-01-01\">\n",
    "    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DL0321EN-SkillsNetwork/image/IDSN-logo.png\" width=\"400\">\n",
    "  </a>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e3d9bb-7ca0-4497-883f-8ceee0420703",
   "metadata": {},
   "source": [
    "<h1 align=left><font size = 6>Lab: Train and Evaluate a Keras-Based Classifier </font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39708660-b3bd-421a-ba68-5c300fe445fb",
   "metadata": {},
   "source": [
    "<h5>Estimated time: 90 minutes</h5>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afc14f2-0c74-49cf-ade7-020ee372ea87",
   "metadata": {},
   "source": [
    "<h2>Objective</h2>\n",
    "\n",
    "\n",
    "After completing this lab, you will be able to:\n",
    "<ul> \n",
    "    \n",
    "1. Create a Keras-based convolutional neural network (CNN) model.\n",
    "2. Train the CNN model on agricultural and non-agricultural land dataset.\n",
    "3. Evaluate the performance of the CNN model. \n",
    "    \n",
    "</ul> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a47bc76-d95e-4eef-8f85-ff1c1ea1fbbb",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook demonstrates the process of building, training, and evaluating a **Keras-based convolutional neural network (CNN)** for image classification, for agricultural images in our case. This lab will cover the following:\n",
    "1. Data preparation\n",
    "2. Model architecture definition\n",
    "3. Training\n",
    "4. Model performance analysis.\n",
    "\n",
    "The goal is to classify satellite images into two categories: \"agricultural\" and \"non-agricultural\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529b3315-53ef-4edd-9906-c48cf6d8b285",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "<font size = 3> \n",
    "\n",
    "1. [Configuration and library imports](#Configuration-and-library-imports)\n",
    "2. [Data acquisition and preparation](#Data-acquisition-and-preparation)\n",
    "3. [Model definition and compilation](#Model-definition-and-compilation)\n",
    "4. [Model training](#Model-training)\n",
    "5. [Download and save the model](#Download-and-save-the-trained-model)\n",
    "6. [Model evaluation and visualization](#Model-evaluation-and-visualization)\n",
    "\n",
    "</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d980e7",
   "metadata": {},
   "source": [
    "## Configuration and library imports\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654b2c08-19fa-4100-a133-3b9ff87a834e",
   "metadata": {},
   "source": [
    "### Install required libraries\n",
    "\n",
    "Some of the required libraries are __not__ pre-installed in the Skills Network Labs environment. You must run the following cell to install them; it might take a few minutes for the installation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebb28acb-eb30-43ec-ad4d-ed929accf500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to check for successful installation of the libraries\n",
    "def lib_installation_check(captured_data, n_lines_print):\n",
    "    \"\"\"\n",
    "    A function to use the %%capture output from the cells where we try to install the libraries.\n",
    "    It would print last \"n_lines_print\" if there is an error in library installation\n",
    "    \"\"\"\n",
    "    output_text = captured_data.stdout\n",
    "    lines = output_text.splitlines()\n",
    "    output_last_n_lines = '\\n'.join(lines[-n_lines_print:])\n",
    "    if \"error\" in output_last_n_lines.lower():\n",
    "        print(\"Library installation failed!\")\n",
    "        print(\"--- Error Details ---\")\n",
    "        print(output_last_n_lines)\n",
    "    else:\n",
    "        print(\"Library installation was successful, let's proceed ahead\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "784459c1-e4cc-4364-a7fc-a0158cf5a9ff",
   "metadata": {},
   "source": [
    "### Library installation - 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a973522-25e8-4411-8fc0-f556e5368de7",
   "metadata": {},
   "source": [
    "Next, let’s install the non-AI libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2670034-d505-4c6e-8447-4f2ce3f1c722",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.6 ms, sys: 20.6 ms, total: 36.2 ms\n",
      "Wall time: 4.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture captured_output\n",
    "%pip install numpy==1.26\n",
    "%pip install matplotlib==3.9.2\n",
    "%pip install skillsnetwork"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b98e265-dd43-42c9-8cd7-ba7e9458ba5e",
   "metadata": {},
   "source": [
    "Now, check if the above libraries are installed properly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d3b63ba-cdda-411c-8dc0-0f8d2cd528c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library installation was successful, let's proceed ahead\n"
     ]
    }
   ],
   "source": [
    "lib_installation_check(captured_data = captured_output, n_lines_print = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b935986-bf16-4b71-96f6-2761f2f3ff08",
   "metadata": {},
   "source": [
    "### `TensorFlow` library installation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4743fb43-7101-460c-afc7-b3bacbaf2354",
   "metadata": {},
   "source": [
    "Next, install the `TensorFlow` library using the code below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6beced9-f72c-4dc0-9c5e-a053095c4d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.19 in /opt/conda/lib/python3.12/site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (2.4.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (25.12.19)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (0.7.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (3.4.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (24.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (5.29.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (2.32.3)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (75.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (2.1.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (1.78.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (3.13.2)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (1.26.0)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (3.15.1)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /opt/conda/lib/python3.12/site-packages (from tensorflow==2.19) (0.5.4)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow==2.19) (0.45.1)\n",
      "Requirement already satisfied: rich in /opt/conda/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow==2.19) (14.3.2)\n",
      "Requirement already satisfied: namex in /opt/conda/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow==2.19) (0.1.0)\n",
      "Requirement already satisfied: optree in /opt/conda/lib/python3.12/site-packages (from keras>=3.5.0->tensorflow==2.19) (0.18.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.19) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.19) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.19) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.19) (2024.12.14)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow==2.19) (3.10.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /opt/conda/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow==2.19) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /opt/conda/lib/python3.12/site-packages (from tensorboard~=2.19.0->tensorflow==2.19) (3.1.5)\n",
      "Requirement already satisfied: markupsafe>=2.1.1 in /opt/conda/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow==2.19) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow==2.19) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich->keras>=3.5.0->tensorflow==2.19) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow==2.19) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "CPU times: user 22.2 ms, sys: 10.1 ms, total: 32.3 ms\n",
      "Wall time: 1.51 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%pip install tensorflow==2.19"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7daaf1f-8e40-4bb4-bf84-55e0231e7061",
   "metadata": {},
   "source": [
    "### `scikit-learn` library installation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a370c3-e436-40f9-846f-7c3a0aabfedd",
   "metadata": {},
   "source": [
    "Install the scikit-learn library. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a71610f-410e-402c-8fc4-83e487d71ba8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn==1.7.0\n",
      "  Downloading scikit_learn-1.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /opt/conda/lib/python3.12/site-packages (from scikit-learn==1.7.0) (1.26.0)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn==1.7.0)\n",
      "  Downloading scipy-1.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (62 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn==1.7.0)\n",
      "  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn==1.7.0)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting numpy>=1.22.0 (from scikit-learn==1.7.0)\n",
      "  Downloading numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Downloading scikit_learn-1.7.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.5/12.5 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "Downloading scipy-1.17.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (35.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.0/35.0 MB\u001b[0m \u001b[31m189.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m154.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, numpy, joblib, scipy, scikit-learn\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.0\n",
      "    Uninstalling numpy-1.26.0:\n",
      "      Successfully uninstalled numpy-1.26.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.4.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed joblib-1.5.3 numpy-2.4.2 scikit-learn-1.7.0 scipy-1.17.0 threadpoolctl-3.6.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn==1.7.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abde24a-e929-4a2f-80d2-4dfc2735d140",
   "metadata": {},
   "source": [
    "### Import libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad78903-2d49-410c-900c-eacac5bfeff1",
   "metadata": {},
   "source": [
    "Import the non-AI libraries. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a75d5a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d385b445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import shutil\n",
    "import random\n",
    "import numpy as np\n",
    "import skillsnetwork\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c356149",
   "metadata": {},
   "source": [
    "### TensorFlow environment configuration\n",
    "\n",
    "This cell sets environment variables for TensorFlow. \n",
    "- `TF_ENABLE_ONEDNN_OPTS` is set to \"0\" to disable Intel oneDNN optimizations, which can sometimes lead to issues or unwanted behavior on specific hardware configurations.\n",
    "- `TF_CPP_MIN_LOG_LEVEL` is set to \"2,\" instructing TensorFlow to only display warning and error messages from its C++ backend. This reduces verbose output and keeps the console cleaner, focusing on more critical information during model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "119fe8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bfe70f",
   "metadata": {},
   "source": [
    "Next, set up the data extraction directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e40eeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_dir = \".\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ace2d9c",
   "metadata": {},
   "source": [
    "## Data acquisition and preparation\n",
    "\n",
    "### Define the dataset URL\n",
    "\n",
    "\n",
    "We define the `url` that holds the link to the dataset. The dataset is a `.tar` archive hosted on a cloud object storage service. Cloud object storage (such as S3) is a highly scalable and durable way to store and retrieve large amounts of unstructured data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a30f5074",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/4Z1fwRR295-1O3PMQBH6Dg/images-dataSAT.tar\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4712e2",
   "metadata": {},
   "source": [
    "### Download the data\n",
    "\n",
    "1. Download and extract data from the cloud using `skillsnetwork.prepare` method.\n",
    "2. Use a fallback method if the `skillsnetwork.prepare` command fails to download and extract the dataset. The fallback involves asynchronously downloading the `.tar` file using `httpx` and then extracting its contents using the `tarfile` library.\n",
    "3. The `tarfile` module provides an interface to tar archives, supporting various compression formats such as gzip and bzip2 (handled by `r:*` mode).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "38e2b732-75ad-4fd1-b892-222e8208a224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_skillnetwork_extraction(extract_dir):\n",
    "    \"\"\" function to check whether data download and extraction method \n",
    "    `skillsnetwork.prepare` would execute successfully, without downloading any data.\n",
    "    This helps in early detection and fast fallback to explicit download and extraction\n",
    "    using default libraries\n",
    "    ###This is a hack for the code to run on non-cloud computing environment without errors\n",
    "    \"\"\"\n",
    "    symlink_test = os.path.join(extract_dir, \"symlink_test\")\n",
    "    if not os.path.exists(symlink_test):\n",
    "        os.symlink(os.path.join(os.sep, \"tmp\"), symlink_test) \n",
    "        print(\"Write permissions available for downloading and extracting the dataset tar file\")\n",
    "    os.unlink(symlink_test)\n",
    "\n",
    "async def download_tar_dataset(url, tar_path, extract_dir):\n",
    "    \"\"\"function to explicitly download and extract the dataset tar file from cloud using native python libraries\n",
    "    \"\"\"\n",
    "    if not os.path.exists(tar_path): # download only if file not downloaded already\n",
    "        try:\n",
    "            print(f\"Downloading from {url}...\")\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                response = await client.get(url, follow_redirects=True)# Download the file asynchronously\n",
    "                response.raise_for_status() # Raise an exception for bad status codes (4xx or 5xx)\n",
    "            \n",
    "                with open(tar_path , \"wb\") as f:\n",
    "                    f.write(response.content) # Save the downloaded file\n",
    "                print(f\"Successfully downloaded '{file_name}'.\")\n",
    "        except httpx.HTTPStatusError as http_err:\n",
    "            print(f\"HTTP error occurred during download: {http_err}\")\n",
    "        except Exception as download_err:\n",
    "            print(f\"An error occurred during the fallback process: {download_err}\")\n",
    "    else:\n",
    "        print(f\"dataset tar file already downloaded at: {tar_path}\")\n",
    "    with tarfile.open(tar_path, 'r:*') as tar_ref:\n",
    "        tar_ref.extractall(path=extract_dir)\n",
    "    print(f\"Successfully extracted to '{extract_dir}'.\")\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2eea70fc-ca44-474f-950c-b8ffd86ca779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write permissions available for downloading and extracting the dataset tar file\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "040896079db24903a4853f674adc1980",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading images-dataSAT.tar:   0%|          | 0/20243456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a86106341bca4c0b9def07edfd9949a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6003 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    check_skillnetwork_extraction(extract_dir)\n",
    "    await skillsnetwork.prepare(url = url, path = extract_dir, overwrite = True)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    # --- FALLBACK METHOD FOR DOWNLOADING THE DATA ---\n",
    "    print(\"Primary download/extration method failed.\")\n",
    "    print(\"Falling back to manual download and extraction...\")\n",
    "    \n",
    "    # import libraries required for downloading and extraction\n",
    "    import tarfile\n",
    "    import httpx \n",
    "    from pathlib import Path\n",
    "    \n",
    "    file_name = Path(url).name# Get the filename from the URL (for example, 'data.tar')\n",
    "    tar_path = os.path.join(extract_dir, file_name)\n",
    "    print(f\"tar_path: {os.path.exists(tar_path)} ___ {tar_path}\")\n",
    "    await download_tar_dataset(url, tar_path, extract_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfbf20f",
   "metadata": {},
   "source": [
    "### Import deep learning and ML libraries\n",
    "\n",
    "Here is a brief description of the usage of the **Keras** libraries and methods that will be used:\n",
    "- `Sequential` models are a linear stack of layers.\n",
    "- `Conv2D` and `MaxPooling2D` are fundamental for CNNs, extracting features and reducing dimensionality.\n",
    "- `BatchNormalization` stabilizes training.\n",
    "- `Dense` layers form the classifier.\n",
    "- `Dropout` regularizes to prevent overfitting.\n",
    "- `Adam` is an adaptive learning rate optimizer.\n",
    "- `ImageDataGenerator` automates data loading and augmentation.\n",
    "- `HeUniform` is used for weight initialization.\n",
    "\n",
    "\n",
    "**Scikit-learn** (`sklearn.metrics`) provides the following metrics for model performance assessment: \n",
    "- `classification_report`\n",
    "- `confusion_matrix`\n",
    "- `accuracy_score`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d220f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1771361445.932015     813 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1771361445.938726     813 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1771361445.954428     813 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771361445.954449     813 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771361445.954451     813 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1771361445.954453     813 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully imported the libraries\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.initializers import HeUniform\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Succesfully imported the libraries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "003882d4-4301-4dde-a0b6-a071533b0766",
   "metadata": {},
   "source": [
    "### Get the processing device\n",
    "Check the availability of GPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2908110-98d2-4890-b7f7-5a11f82ad859",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available for training: cpu\n"
     ]
    }
   ],
   "source": [
    "gpu_list = tf.config.list_physical_devices('GPU')\n",
    "\n",
    "device = \"gpu\" if gpu_list !=[] else \"cpu\"\n",
    "print(f\"Device available for training: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de9da38a",
   "metadata": {},
   "source": [
    "### Reproducibility with random seeds\n",
    "\n",
    "Here we fix the random seeds for `random` module, NumPy, and TensorFlow. By initializing these seeds with a constant value (for example, 42), any operations that involve randomness (such as weight initialization, data shuffling, or data augmentation) will produce the exact same sequence of random numbers every time the code is run. This is crucial for ensuring the reproducibility of experimental results and when comparing different models or hyperparameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82346e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "seed_value = 7331\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "tf.random.set_seed(seed_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc13d4d",
   "metadata": {},
   "source": [
    "### Define the dataset path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a5f6c8eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./images_dataSAT\n"
     ]
    }
   ],
   "source": [
    "dataset_path = os.path.join(extract_dir, \"images_dataSAT\")\n",
    "print(dataset_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcda9646",
   "metadata": {},
   "source": [
    "### Create the dataset file list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4084fba0-b7b1-4a5c-baa6-c88a0ff34f6b",
   "metadata": {},
   "source": [
    "Now that we have downloaded the dataset, perform the following task. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61942ff0-5514-4b3d-8173-861885df05b3",
   "metadata": {},
   "source": [
    "### **Task 1:** Recursively walk through the `dataset_path` using `os.walk` function to create a list **`fnames`** of all image files. \n",
    "Print the total count of files found and displays the first two and last two file paths. \n",
    "\n",
    "Absolute path is captured using `os.path.join(dirname, filename)` and used in `ImageDataGenerator` later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6a2f2b69-ee38-4174-b86f-694d6377a84d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total files in dataset: 6000\n",
      "./images_dataSAT/class_0_non_agri/tile_S2A_MSIL2A_20250427T101701_N0511_R065_T32UQB_20250427T170513.SAFE_15431.jpg\n",
      "./images_dataSAT/class_0_non_agri/tile_S2A_MSIL2A_20250427T101701_N0511_R065_T32UPE_20250427T170513.SAFE_14713.jpg\n",
      "./images_dataSAT/class_1_agri/tile_S2A_MSIL2A_20250427T101701_N0511_R065_T32UQB_20250427T170513.SAFE_12587.jpg\n",
      "./images_dataSAT/class_1_agri/tile_S2A_MSIL2A_20250427T101701_N0511_R065_T32UQB_20250427T170513.SAFE_3371.jpg\n"
     ]
    }
   ],
   "source": [
    "## You can use this cell to type the code to complete the task.\n",
    "\n",
    "fnames = []\n",
    "for dirname, _, filenames in os.walk(dataset_path):\n",
    "    for filename in filenames:\n",
    "        fnames.append(os.path.join(dirname, filename))\n",
    "\n",
    "print(f\"total files in dataset: {len(fnames)}\")\n",
    "nfname_print=2\n",
    "for f in fnames[:nfname_print]:\n",
    "    print(f)\n",
    "for f in fnames[-nfname_print:]:\n",
    "    print(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9398d118-7eac-41e4-aaa1-3771afcb82fe",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "\n",
    "\n",
    "fnames = []\n",
    "for dirname, _, filenames in os.walk(dataset_path):\n",
    "    for filename in filenames:\n",
    "        fnames.append(os.path.join(dirname, filename))\n",
    "print(f\"total files in dataset: {len(fnames)}\")\n",
    "nfname_print=2\n",
    "for f in fnames[:nfname_print]:\n",
    "    print(f)\n",
    "for f in fnames[-nfname_print:]:\n",
    "    print(f)\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb34fa2a",
   "metadata": {},
   "source": [
    "### Define the model hyperparameters\n",
    "\n",
    "Hyperparameters are configurable values that are set before the training process begins. \n",
    "\n",
    "This cell initializes several key hyperparameters that will govern the training process and the model's input. Here is the list of hyperparameters:\n",
    "\n",
    "1. `img_w` and `img_h` define the width and height for resizing input images.\n",
    "2. `n_channels` defines the number of color channels (3 for RGB).\n",
    "3. `n_epochs` sets the total training iterations over the dataset.\n",
    "4. `batch_size` sets the number of samples processed per batch in the epoch.\n",
    "5. `lr` defines the learning rate for the optimizer.\n",
    "6. `steps_per_epoch` are total number of steps used for training. **None** means the number is calculated automatically.\n",
    "7. `validation_steps` are total number of steps used for validating the model on validation data. **None** means the number is calculated automatically.\n",
    "\n",
    "These hyperparameters are crucial for controlling model performance and resource utilization and significantly influence a model's performance and training efficiency. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97eae79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_w, img_h = 64, 64\n",
    "n_channels = 3\n",
    "batch_size = 128\n",
    "lr = 0.001 # Learning rate\n",
    "n_epochs = 3 # set to low number for your convenience. You can change this to any number of your liking\n",
    "\n",
    "steps_per_epoch = None\n",
    "validation_steps = None \n",
    "\n",
    "model_name = \"ai_capstone_keras_best_model.model.keras\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aba0671",
   "metadata": {},
   "source": [
    "### Configure `ImageDataGenerator` for Augmentation\n",
    "\n",
    "\n",
    "Now, we instantiate the `ImageDataGenerator` with data augmentation parameters:\n",
    "\n",
    "- `rescale=1./255` normalizes pixel values to [0, 1].\n",
    "- `rotation_range`, `width_shift_range`, `height_shift_range`, `shear_range`, and `zoom_range` define random transformations to apply to images during training, increasing dataset diversity.\n",
    "- `horizontal_flip=True` enables random horizontal mirroring.\n",
    "- `fill_mode='nearest'` specifies how new pixels are filled after transformations.\n",
    "- `validation_split=0.2` reserves 20% of data for validation.\n",
    "\n",
    "\n",
    "This setup boosts model robustness against variations in real-world images. `ImageDataGenerator` performs these transformations on-the-fly, making it efficient for large datasets. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "88cc5bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1./255,\n",
    "                             rotation_range=40, \n",
    "                             width_shift_range=0.2,\n",
    "                             height_shift_range=0.2,\n",
    "                             shear_range=0.2,\n",
    "                             zoom_range=0.2,\n",
    "                             horizontal_flip=True,\n",
    "                             fill_mode=\"nearest\",\n",
    "                             validation_split=0.2\n",
    "                            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44d2ce28",
   "metadata": {},
   "source": [
    "### Create training and validation data generators\n",
    "\n",
    " `ImageDataGenerator` is used to create `train_generator` and `validation_generator`. \n",
    "`flow_from_directory()` is a convenient method of `ImageDataGenerator` for automatically creating data pipelines from structured image directories.\n",
    " The generator resize images to `(img_w, img_h)` and group them into `batch_size` chunks. `class_mode=\"binary\"` indicates a two-class classification task. \n",
    " \n",
    " The `subset` parameter is used to assign 80% of the data for training and 20% for validation based on the `validation_split`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57ff1974-13dc-4fce-842b-041edbb2cfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4800 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = datagen.flow_from_directory(dataset_path,\n",
    " target_size = (img_w, img_h),\n",
    " batch_size= batch_size,\n",
    " class_mode=\"binary\",\n",
    " subset=\"training\"\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c25805f5-962c-42dc-9d90-cb95065a101e",
   "metadata": {},
   "source": [
    "Here is your next task. We have created the `train_generator`, let's create the `validation_generator`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e7b840-63d3-4881-9a65-8f0b9f8d9894",
   "metadata": {},
   "source": [
    "### **Task 2:** Create the `validation_generator` from `dataset_path`.\n",
    "Use `target_size`, and `class_mode` similar to `train_generator`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3801e435-d4bf-4630-ab47-24f8c75c6d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1200 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "## You can use this cell to type the code to complete the task.\n",
    "\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(dataset_path,\n",
    "                                                    target_size =(img_w, img_h),\n",
    "                                                    batch_size = batch_size, \n",
    "                                                    class_mode=\"binary\",\n",
    "                                                    subset=\"validation\"\n",
    "                                                    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa6cafb-8064-40a1-83c2-c1a370c00976",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "validation_generator = datagen.flow_from_directory(dataset_path,\n",
    "                                                    target_size =(img_w, img_h),\n",
    "                                                    batch_size = batch_size, \n",
    "                                                    class_mode=\"binary\",\n",
    "                                                    subset=\"validation\"\n",
    "                                                    )\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df897734",
   "metadata": {},
   "source": [
    "## Model definition and compilation\n",
    "\n",
    "### Define the convolutional neural network (CNN) architecture\n",
    "\n",
    "The model architecture is composed of several key components:\n",
    "- **`Sequential`** is a linear stack of layers in Keras.\n",
    "- **Conv2D** layers perform convolution operations, acting as feature detectors.\n",
    "- **MaxPooling2D** reduces the spatial dimensions of the feature maps.\n",
    "-  **BatchNormalization** normalizes layer inputs, stabilizing and accelerating training.\n",
    "-  **GlobalAveragePooling2D** summarizes feature maps into a single vector, reducing parameters.\n",
    "-  **Dense** (fully connected) layers learn complex patterns from these features.\n",
    "-  **Dropout** is a regularization technique that randomly deactivates neurons during training.\n",
    "-  **Sigmoid** activation is used for binary classification, mapping outputs to probabilities.\n",
    "-  **HeUniform** initializer is suitable for ReLU activations.\n",
    "-  **The final output `Dense` layer** uses a `sigmoid` activation for binary classification, outputting a probability between 0 and 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7197185c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "                    Conv2D(32 , (5,5) , activation=\"relu\",padding=\"same\",strides=(1,1), kernel_initializer=HeUniform(), input_shape=(img_w, img_h, n_channels)),\n",
    "                    MaxPooling2D(2,2),\n",
    "                    BatchNormalization(),\n",
    "                    \n",
    "                    Conv2D(64, (5,5) , activation=\"relu\",padding=\"same\" , strides=(1,1), kernel_initializer=HeUniform()),\n",
    "                    MaxPooling2D(2,2),\n",
    "                    BatchNormalization(),\n",
    "                    \n",
    "                    Conv2D(128, (5,5) , activation=\"relu\",padding=\"same\" ,strides=(1,1), kernel_initializer=HeUniform()),\n",
    "                    MaxPooling2D(2,2),\n",
    "                    BatchNormalization(),\n",
    "                    \n",
    "                    ###\n",
    "                    Conv2D(256, (5,5) , activation=\"relu\",padding=\"same\" ,strides=(1,1), kernel_initializer=HeUniform()),\n",
    "                    MaxPooling2D(2,2),\n",
    "                    BatchNormalization(),\n",
    "                    \n",
    "                    Conv2D(512, (5,5) , activation=\"relu\",padding=\"same\" ,strides=(1,1), kernel_initializer=HeUniform()),\n",
    "                    MaxPooling2D(2,2),\n",
    "                    BatchNormalization(),\n",
    "                    \n",
    "                    Conv2D(1024, (5,5) , activation=\"relu\",padding=\"same\" ,strides=(1,1), kernel_initializer=HeUniform()),\n",
    "                    MaxPooling2D(2,2),\n",
    "                    BatchNormalization(),\n",
    "                    \n",
    "                    \n",
    "                    ###\n",
    "                    GlobalAveragePooling2D(),\n",
    "                    \n",
    "                    Dense(64,activation=\"relu\" , kernel_initializer=HeUniform()),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(0.4),\n",
    "                    \n",
    "                    Dense(128,activation=\"relu\" , kernel_initializer=HeUniform()),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(0.4),\n",
    "                    \n",
    "                    Dense(256,activation=\"relu\" , kernel_initializer=HeUniform()),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(0.4),\n",
    "                    \n",
    "                    ###\n",
    "                    Dense(512,activation=\"relu\" , kernel_initializer=HeUniform()),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(0.4),\n",
    "                    \n",
    "                    Dense(1024,activation=\"relu\" , kernel_initializer=HeUniform()),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(0.4),\n",
    "                    \n",
    "                    Dense(2048,activation=\"relu\" , kernel_initializer=HeUniform()),\n",
    "                    BatchNormalization(),\n",
    "                    Dropout(0.4),\n",
    "                    \n",
    "                    \n",
    "                    ###\n",
    "                    Dense(1 , activation=\"sigmoid\")\n",
    "                    \n",
    "                ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c59dda6",
   "metadata": {},
   "source": [
    "### Compile the model and display the summary\n",
    "\n",
    "\n",
    "Here, we compile the model using `model.compile()` with the `Adam` optimizer and `learning_rate` equal to `lr` (0.001). \n",
    "\n",
    "The `loss` function is specified as `\"binary_crossentropy\"`, appropriate for binary classification problems. \n",
    "`accuracy` is set as the performance `metric` to monitor training and evaluation. \n",
    "We print `model.summary()` for a detailed overview of the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cc7a234a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,432</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">51,264</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │       <span style=\"color: #00af00; text-decoration-color: #00af00\">204,928</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │       <span style=\"color: #00af00; text-decoration-color: #00af00\">819,456</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,277,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)      │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">13,108,224</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">65,600</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │           <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,024</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">131,584</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,048</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">4,096</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,099,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,192</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,049</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m64\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │         \u001b[38;5;34m2,432\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m32\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m51,264\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │       \u001b[38;5;34m204,928\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_2           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │       \u001b[38;5;34m819,456\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_3           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m256\u001b[0m)      │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │     \u001b[38;5;34m3,277,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_4           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m512\u001b[0m)      │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │    \u001b[38;5;34m13,108,224\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_5           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │         \u001b[38;5;34m4,096\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling2d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m65,600\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_6           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │           \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m8,320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_7           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │           \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_8           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │         \u001b[38;5;34m1,024\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │       \u001b[38;5;34m131,584\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_9           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │         \u001b[38;5;34m2,048\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_3 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │       \u001b[38;5;34m525,312\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_10          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │         \u001b[38;5;34m4,096\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │     \u001b[38;5;34m2,099,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_11          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │         \u001b[38;5;34m8,192\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_6 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │         \u001b[38;5;34m2,049\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,352,897</span> (77.64 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m20,352,897\u001b[0m (77.64 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">20,340,801</span> (77.59 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m20,340,801\u001b[0m (77.59 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">12,096</span> (47.25 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m12,096\u001b[0m (47.25 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "loss = \"binary_crossentropy\"\n",
    "model.compile(optimizer=Adam(learning_rate=lr),\n",
    "              loss=loss, \n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602ace3c-0159-44f9-a386-6e2586f0d90c",
   "metadata": {},
   "source": [
    "Answer the question below in the space provided. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f97e78c1-121b-4461-a440-2505bf8faa48",
   "metadata": {},
   "source": [
    "## Question: Count the total number of layers in this CNN model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5420f7c-8738-476c-bea7-db19a38a395e",
   "metadata": {},
   "source": [
    "### You can use this cell to type the answer to the question.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3be625e-e99d-454f-a945-4d788df6578f",
   "metadata": {},
   "source": [
    "There are total of 38 Layers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbffce73-7f2d-4cd2-8ca3-58f5e6480587",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "There are total 38 layers.\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce0cf5c-d6b5-4889-be90-0aa6c61be7fc",
   "metadata": {},
   "source": [
    "You now know how to create a CNN model using Keras. You can perform the following task. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc991a22-1b16-4dc8-add3-cb36d194942c",
   "metadata": {},
   "source": [
    "## Task 3: Create and compile a CNN model `test_model` with 4 Conv2D layers and 5 Dense layers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a84fd320-0dd2-4b37-a223-2f3bb37583ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can use this cell to type the code to complete the task.\n",
    "\n",
    "absmodel = Sequential([\n",
    " Conv2D(32 , (5,5) , activation=\"relu\",padding=\"same\",strides=(1,1), kernel_initializer=HeUniform(), input_shape=(img_w, img_h, n_channels)),\n",
    " MaxPooling2D(2,2),\n",
    " BatchNormalization(),\n",
    "\n",
    " Conv2D(64, (5,5) , activation=\"relu\",padding=\"same\" , strides=(1,1), kernel_initializer=HeUniform()),\n",
    " MaxPooling2D(2,2),\n",
    " BatchNormalization(),\n",
    "\n",
    " Conv2D(128, (5,5) , activation=\"relu\",padding=\"same\" ,strides=(1,1), kernel_initializer=HeUniform()),\n",
    " MaxPooling2D(2,2),\n",
    " BatchNormalization(),\n",
    " \n",
    "\n",
    " Conv2D(256, (5,5) , activation=\"relu\",padding=\"same\" ,strides=(1,1), kernel_initializer=HeUniform()),\n",
    " MaxPooling2D(2,2),\n",
    " BatchNormalization(),\n",
    " \n",
    "\n",
    " GlobalAveragePooling2D(),\n",
    "\n",
    " Dense(64,activation=\"relu\" , kernel_initializer=HeUniform()),\n",
    " BatchNormalization(),\n",
    " Dropout(0.4),\n",
    "\n",
    " Dense(128,activation=\"relu\" , kernel_initializer=HeUniform()),\n",
    " BatchNormalization(),\n",
    " Dropout(0.4),\n",
    "\n",
    " Dense(256,activation=\"relu\" , kernel_initializer=HeUniform()),\n",
    " BatchNormalization(),\n",
    " Dropout(0.4),\n",
    "\n",
    "\n",
    " Dense(512,activation=\"relu\" , kernel_initializer=HeUniform()),\n",
    " BatchNormalization(),\n",
    " Dropout(0.4),\n",
    "\n",
    "\n",
    " Dense(1 , activation=\"sigmoid\")\n",
    " \n",
    " ])\n",
    "\n",
    "\n",
    "loss = \"binary_crossentropy\"\n",
    "model.compile(optimizer=Adam(learning_rate=lr),loss=loss, metrics=[\"accuracy\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3dd6c52-9b24-4184-a133-f6c78203260b",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "model = Sequential([\n",
    " Conv2D(32 , (5,5) , activation=\"relu\",padding=\"same\",strides=(1,1), kernel_initializer=HeUniform(), input_shape=(img_w, img_h, n_channels)),\n",
    " MaxPooling2D(2,2),\n",
    " BatchNormalization(),\n",
    "\n",
    " Conv2D(64, (5,5) , activation=\"relu\",padding=\"same\" , strides=(1,1), kernel_initializer=HeUniform()),\n",
    " MaxPooling2D(2,2),\n",
    " BatchNormalization(),\n",
    "\n",
    " Conv2D(128, (5,5) , activation=\"relu\",padding=\"same\" ,strides=(1,1), kernel_initializer=HeUniform()),\n",
    " MaxPooling2D(2,2),\n",
    " BatchNormalization(),\n",
    " \n",
    "###\n",
    " Conv2D(256, (5,5) , activation=\"relu\",padding=\"same\" ,strides=(1,1), kernel_initializer=HeUniform()),\n",
    " MaxPooling2D(2,2),\n",
    " BatchNormalization(),\n",
    " \n",
    "###\n",
    " GlobalAveragePooling2D(),\n",
    "\n",
    " Dense(64,activation=\"relu\" , kernel_initializer=HeUniform()),\n",
    " BatchNormalization(),\n",
    " Dropout(0.4),\n",
    "\n",
    " Dense(128,activation=\"relu\" , kernel_initializer=HeUniform()),\n",
    " BatchNormalization(),\n",
    " Dropout(0.4),\n",
    "\n",
    " Dense(256,activation=\"relu\" , kernel_initializer=HeUniform()),\n",
    " BatchNormalization(),\n",
    " Dropout(0.4),\n",
    "\n",
    "###\n",
    " Dense(512,activation=\"relu\" , kernel_initializer=HeUniform()),\n",
    " BatchNormalization(),\n",
    " Dropout(0.4),\n",
    "\n",
    "###\n",
    " Dense(1 , activation=\"sigmoid\")\n",
    " \n",
    " ])\n",
    "\n",
    "# Compile the model to make it ready for training\n",
    "loss = \"binary_crossentropy\"\n",
    "model.compile(optimizer=Adam(learning_rate=lr),loss=loss, metrics=[\"accuracy\"])\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7045b4a",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "### Display the training configuration and hyperparameters\n",
    "\n",
    "Here we print a comprehensive summary of the training configuration and list all critical hyperparameters. This detailed output serves as a quick reference and verification of the experimental setup.\n",
    "Before commencing computationally intensive tasks such as deep learning model training, it's a good practice to log and verify the configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9f7cb52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Hyperparameters:\n",
      "        n_classes (train) = 2,\n",
      "        n_classes (validation) = 2,\n",
      "        img_w, img_h =(64, 64),\n",
      "        n_channels = 3,\n",
      "        batch_size = 128,\n",
      "        steps_per_epoch = None,\n",
      "        n_epochs = 3,\n",
      "        validation_steps = None,\n",
      "        learning_rate = 0.001\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training Hyperparameters:\\n\\\n",
    "        n_classes (train) = {train_generator.num_classes},\\n\\\n",
    "        n_classes (validation) = {validation_generator.num_classes},\\n\\\n",
    "        img_w, img_h ={img_w, img_h},\\n\\\n",
    "        n_channels = {n_channels},\\n\\\n",
    "        batch_size = {batch_size},\\n\\\n",
    "        steps_per_epoch = {steps_per_epoch},\\n\\\n",
    "        n_epochs = {n_epochs},\\n\\\n",
    "        validation_steps = {validation_steps},\\n\\\n",
    "        learning_rate = {lr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2043d82b-6bfb-4fab-bfc2-69c0d943c491",
   "metadata": {},
   "source": [
    "### Save the model checkpoint\n",
    "\n",
    "Now we declare a method to save the **best model** during training. The best model can be defined by either **lowest loss** or **high accuracy**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5d7030d8-a5fa-4c3d-8da7-e32bf78b0f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the ModelCheckpoint callback\n",
    "checkpoint_cb = ModelCheckpoint(filepath=model_name,\n",
    "                                monitor='val_loss',      # or 'val_accuracy'\n",
    "                                mode='min',              # 'min' for loss, 'max' for accuracy\n",
    "                                save_best_only=True,\n",
    "                                verbose=1\n",
    "                               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd6d845-24ce-4492-9b7a-e7b96798bb08",
   "metadata": {},
   "source": [
    "The checkpoint of a model can also be based on high accuracy. So, here's your next task.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ed3e8b-5ec0-41df-a76c-4239fff66f94",
   "metadata": {},
   "source": [
    "### **Task 4**: Create the checkpoint callback for model with **maximum accuracy**. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e356f45b-0c7d-4403-83f9-16a797cd72fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can use this cell to type the code to complete the task.\n",
    "\n",
    "checkpoint_cb = ModelCheckpoint(filepath=model_name,\n",
    "                                monitor='val_accuracy',\n",
    "                                mode='max',\n",
    "                                save_best_only=True,\n",
    "                                verbose=1\n",
    "                               )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6156341a-60a0-4cd6-84ae-3bbe81262cda",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "checkpoint_cb = ModelCheckpoint(filepath=model_name,\n",
    "                                monitor='val_accuracy',\n",
    "                                mode='max',\n",
    "                                save_best_only=True,\n",
    "                                verbose=1\n",
    "                               )\n",
    "\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6220ec0c",
   "metadata": {},
   "source": [
    "### Execute model training\n",
    "\n",
    "- `model.fit()` is the primary function for training a Keras model. It controls the entire training loop: iterating over epochs, fetching data batches from generators, performing forward and backward passes, updating weights via the optimizer, and calculating loss and metrics.\n",
    "- `steps_per_epoch` (*if specified*) determines how many batches constitute an \"epoch.\"\n",
    "- `validation_data` and `validation_steps` allow monitoring of the model's generalization ability on a separate dataset, helps in detecting overfitting.\n",
    "- `callbacks` determines how the best model is saved.\n",
    "- The `fit` object stores the model's training history.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a9e84af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on : ===cpu=== with batch size: 128 & lr: 0.001\n",
      "Epoch 1/3\n",
      "\u001b[1m13/38\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m21:44\u001b[0m 52s/step - accuracy: 0.6240 - loss: 0.7948"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nFileNotFoundError: [Errno 2] No such file or directory: './images_dataSAT/class_1_agri/tile_S2A_MSIL2A_20250427T101701_N0511_R065_T32UQB_20250427T170513.SAFE_2149.jpg'\nTraceback (most recent call last):\n\n  File \"/opt/conda/lib/python3.12/site-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.12/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py\", line 264, in _finite_generator\n    yield self._standardize_batch(self.py_dataset[i])\n                                  ~~~~~~~~~~~~~~~^^^\n\n  File \"/opt/conda/lib/python3.12/site-packages/keras/src/legacy/preprocessing/image.py\", line 71, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.12/site-packages/keras/src/legacy/preprocessing/image.py\", line 316, in _get_batches_of_transformed_samples\n    img = image_utils.load_img(\n          ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.12/site-packages/keras/src/utils/image_utils.py\", line 247, in load_img\n    with open(path, \"rb\") as f:\n         ^^^^^^^^^^^^^^^^\n\nFileNotFoundError: [Errno 2] No such file or directory: './images_dataSAT/class_1_agri/tile_S2A_MSIL2A_20250427T101701_N0511_R065_T32UQB_20250427T170513.SAFE_2149.jpg'\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_multi_step_on_iterator_11110]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining on : ===\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=== with batch size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m & lr: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m fit \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m                \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m                \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m                \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcheckpoint_cb\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[43m               \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnknownError\u001b[0m: Graph execution error:\n\nDetected at node PyFunc defined at (most recent call last):\n<stack traces unavailable>\nFileNotFoundError: [Errno 2] No such file or directory: './images_dataSAT/class_1_agri/tile_S2A_MSIL2A_20250427T101701_N0511_R065_T32UQB_20250427T170513.SAFE_2149.jpg'\nTraceback (most recent call last):\n\n  File \"/opt/conda/lib/python3.12/site-packages/tensorflow/python/ops/script_ops.py\", line 269, in __call__\n    ret = func(*args)\n          ^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.12/site-packages/tensorflow/python/autograph/impl/api.py\", line 643, in wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.12/site-packages/tensorflow/python/data/ops/from_generator_op.py\", line 198, in generator_py_func\n    values = next(generator_state.get_iterator(iterator_id))\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.12/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py\", line 264, in _finite_generator\n    yield self._standardize_batch(self.py_dataset[i])\n                                  ~~~~~~~~~~~~~~~^^^\n\n  File \"/opt/conda/lib/python3.12/site-packages/keras/src/legacy/preprocessing/image.py\", line 71, in __getitem__\n    return self._get_batches_of_transformed_samples(index_array)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.12/site-packages/keras/src/legacy/preprocessing/image.py\", line 316, in _get_batches_of_transformed_samples\n    img = image_utils.load_img(\n          ^^^^^^^^^^^^^^^^^^^^^\n\n  File \"/opt/conda/lib/python3.12/site-packages/keras/src/utils/image_utils.py\", line 247, in load_img\n    with open(path, \"rb\") as f:\n         ^^^^^^^^^^^^^^^^\n\nFileNotFoundError: [Errno 2] No such file or directory: './images_dataSAT/class_1_agri/tile_S2A_MSIL2A_20250427T101701_N0511_R065_T32UQB_20250427T170513.SAFE_2149.jpg'\n\n\n\t [[{{node PyFunc}}]]\n\t [[IteratorGetNext]] [Op:__inference_multi_step_on_iterator_11110]"
     ]
    }
   ],
   "source": [
    "print(f\"Training on : ==={device}=== with batch size: {batch_size} & lr: {lr}\")\n",
    "\n",
    "fit = model.fit(train_generator, \n",
    "                epochs= n_epochs,\n",
    "                steps_per_epoch = steps_per_epoch,\n",
    "                validation_data=(validation_generator),\n",
    "                validation_steps = validation_steps,\n",
    "                callbacks=[checkpoint_cb],\n",
    "                verbose=1\n",
    "               )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11207c3c-3d98-4294-8f59-4e4eb1456b26",
   "metadata": {},
   "source": [
    "## Download and save the trained model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a5d799-0dcc-4fd1-a586-18196feeb756",
   "metadata": {},
   "source": [
    "After the training is completed, you will see `ai_capstone_keras_best_model.model.keras` in the left pane\n",
    "\n",
    "**However**, for your convenience, I have saved a model trained over 20 epochs **[here](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/f63OXPboUBgVhDpozcJZ3w/ai-capstone-keras-best-model-model.keras)**. You can download that for evaluation and further labs on your local machine from **[this link](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/f63OXPboUBgVhDpozcJZ3w/ai-capstone-keras-best-model-model.keras)**.\n",
    "\n",
    "\n",
    "This is the Keras AI model created by training on the provided dataset for agricultural and non-agricultural land dartaset. This model can now be used for infering un-classified images with the dimensions similar to the training images. \n",
    "\n",
    "- You can also download the your trained model file: `ai_capstone_keras_best_model.model.keras` from the left pane and save it on your local computer. \n",
    "- You can download this model by \"right-click\" on the file and then Clickinng \"Download\".\n",
    "- You could use this model for the other labs of this capstone project course.\n",
    "\n",
    "\n",
    "Please refer to the screenshots below for downloading the model to your local computer.\n",
    "\n",
    "\n",
    "### The trained model file (`ai_capstone_keras_best_model.model.keras` ) in the left pane\n",
    "![Model_Keras_download_screenshot_1_marked.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/NM4wJ1o8G3f0Gv3Ic_cHOQ/Model-Keras-download-screenshot-1-marked.png)\n",
    "\n",
    "\n",
    "### The **download** option\n",
    "![Model_Keras_download_screenshot_2_marked.png](https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/y4ubxvX6OHSWP9KvB-fzHQ/Model-Keras-download-screenshot-2-marked.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c095e5f6-4687-4cb4-9263-4a081f133021",
   "metadata": {},
   "source": [
    "## Model evaluation and visualization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7988c04",
   "metadata": {},
   "source": [
    "### Perform a comprehensive model evaluation\n",
    "\n",
    "Here, you will perform a detailed evaluation of the trained model on the validation dataset. You would calculate the necessary prediction `steps` based on the validation data and `batch_size`. Then, you will obtain the true class labels (`y_true`) and generate the model's predictions (`y_pred`) on the validation set. The predicted probabilities are converted to binary class labels using a 0.5 threshold. Finally, you will print the overall `accuracy_score`, to get a  quantitative assessment of the model's performance on unseen data.\n",
    "\n",
    "Model evaluation metrics are essential for understanding a model's generalization ability. `y_true` represents the actual labels, while `y_pred` are the model's predicted labels. For binary classification, probabilities are converted to class labels by thresholding. The **accuracy score** is the proportion of correct predictions out of the total predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "40c7fe5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 1s/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1s/step\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 1s/step\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3s/step\n",
      "Accuracy Score: 0.5467\n"
     ]
    }
   ],
   "source": [
    "steps = int(np.ceil(validation_generator.samples / validation_generator.batch_size))\n",
    "batch_size = int(validation_generator.batch_size)\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "for step in range(steps):\n",
    "    # Get one batch data\n",
    "    images, labels = next(validation_generator)\n",
    "    preds = model.predict(images)\n",
    "    preds = (preds > 0.5).astype(int).flatten() \n",
    "    all_preds.extend(preds)\n",
    "    all_labels.extend(labels)\n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Accuracy Score: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58fc11a-0228-47dc-ac46-521cd739e1b5",
   "metadata": {},
   "source": [
    "### Visualize the training history (accuracy and loss)\n",
    "\n",
    "\n",
    "This cell generates two plots to visualize the model's training performance, one for accuracy and one for loss, across epochs. \n",
    "- **Accuracy** measures the proportion of correct predictions. \n",
    "- **Loss** quantifies the error between predictions and true labels. \n",
    "- Using these metrics, we can check the model for **overfitting** or **underfitting**. \n",
    "- `fit.history` attribute stores these metrics for each epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b3e85bc7-ce80-4780-a1ee-522aae09bbb9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Plot Accuracy on the first subplot\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m axs\u001b[38;5;241m.\u001b[39mplot(\u001b[43mfit\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m axs\u001b[38;5;241m.\u001b[39mplot(fit\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m axs\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel Accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fit' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a figure with a subplot\n",
    "fig, axs = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plot Accuracy on the first subplot\n",
    "axs.plot(fit.history['accuracy'], label='Training Accuracy')\n",
    "axs.plot(fit.history['val_accuracy'], label='Validation Accuracy')\n",
    "axs.set_title('Model Accuracy')\n",
    "axs.set_xlabel('Epochs')\n",
    "axs.set_ylabel('Accuracy')\n",
    "axs.legend()\n",
    "axs.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd999097-1580-43b6-99c5-6ba956cabdbd",
   "metadata": {},
   "source": [
    "Plot the model loss in the task below. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084ba209-5b62-4686-8353-ec3c6d3f39bd",
   "metadata": {},
   "source": [
    "### **Task 5:** Plot the graph for **training loss** and **validation loss** for the model `fit`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "06249605-0d3c-4a36-b53b-9b07924a9e46",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## You can use this cell to type the code to complete the task.\u001b[39;00m\n\u001b[1;32m      3\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots( figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m----> 7\u001b[0m axs\u001b[38;5;241m.\u001b[39mplot(\u001b[43mfit\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m axs\u001b[38;5;241m.\u001b[39mplot(fit\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m axs\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fit' is not defined"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## You can use this cell to type the code to complete the task.\n",
    "\n",
    "fig, axs = plt.subplots( figsize=(8, 6))\n",
    "\n",
    "\n",
    "\n",
    "axs.plot(fit.history['loss'], label='Training Loss')\n",
    "axs.plot(fit.history['val_loss'], label='Validation Loss')\n",
    "axs.set_title('Model Loss')\n",
    "axs.set_xlabel('Epochs')\n",
    "axs.set_ylabel('Loss')\n",
    "axs.legend()\n",
    "axs.grid(True)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3143eeb9-e305-4a39-9598-2bdaf84115c4",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!-- The correct answer is:\n",
    "\n",
    "fig, axs = plt.subplots( figsize=(8, 6))\n",
    "\n",
    "\n",
    "# Plot Loss on the second subplot\n",
    "axs.plot(fit.history['loss'], label='Training Loss')\n",
    "axs.plot(fit.history['val_loss'], label='Validation Loss')\n",
    "axs.set_title('Model Loss')\n",
    "axs.set_xlabel('Epochs')\n",
    "axs.set_ylabel('Loss')\n",
    "axs.legend()\n",
    "axs.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45da1a7d-3166-4ded-b9d2-9c46a759e3d6",
   "metadata": {},
   "source": [
    "## Save and download the notebook for **final project** submission and evaluation\n",
    "\n",
    "You will need to save and download the completed notebook for final project submission and evaluation. \n",
    "<br>For saving and downloading the completed ntoebook, please follow the steps given below:</br>\n",
    "\n",
    "<font size = 4>  \n",
    "\n",
    "1) **Complete** all the tasks and questions given in the notebook.\n",
    "\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/nv4jHlPU5_R1q7ZJrZ69eg/DL0321EN-M1L1-Save-IPYNB-Screenshot-1.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "2) **Save** the notebook.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/9-WPWD4mW1d-RV5Il5otTg/DL0321EN-M1L1-Save-IPYNB-Screenshot-2.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "3) Identify and right click on the **correct notebook file** in the left pane.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/RUSRPw7NT6Sof94B7-9naQ/DL0321EN-M1L1-Save-IPYNB-Screenshot-3.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "4) Click on **Download**.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/HHry4GT-vhLEcRi1T_LHGg/DL0321EN-M1L1-Save-IPYNB-Screenshot-4.png\" style=\"width:600px; border:0px solid black;\">\n",
    "\n",
    "5) Download and **Save** the Jupyter notebook file on your computer **for final submission**.</style>\n",
    "<img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/hhsJbxc6R-T8_pXQGjMjvg/DL0321EN-M1L1-Save-IPYNB-Screenshot-5.png\" style=\"width:600px; border:0px solid black;\">\n",
    "  </font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ce0319-3189-4db3-aeb7-d8ef19734030",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "Congratulation! You've successfully bulit, trained, and evaluated a deep learning model using Keras for image classification.\n",
    "\n",
    "- **Robust data handling:** We implemented a robust data acquisition strategy, featuring a primary method and a crucial fallback for reliable data downloading and extraction.\n",
    "- **Reproducibility:** We used fixed random seeds ensures your results are consistent and verifiable across multiple runs.\n",
    "- **Data generators:** We learnt about ImageDataGenerator for efficient on-the-fly image loading, resizing, normalization, and vital data augmentation.\n",
    "- **CNN architecture:** We built a multi-layered CNN, incorporating Conv2D, MaxPooling2D, BatchNormalization, Dropout, and Dense layers for effective feature learning and classification.\n",
    "- **Model compilation:** We configured the model's learning process with an Adam optimizer, binary_crossentropy loss, and accuracy metric.\n",
    "- **Training process:** We executed the training loop, feeding data in batches and monitoring performance over epochs.\n",
    "- **Performance visualization:** We plotted the accuracy and loss plots for understandinbg the model's learning progress and identify overfitting.\n",
    "- **Model evaluation:** Finally, we use accuracy_score for a quantitative assessment of your model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "807370f5-c80c-4498-9955-6e15aad4bdbc",
   "metadata": {},
   "source": [
    "<h2>Author</h2>\n",
    "\n",
    "[Aman Aggarwal](https://www.linkedin.com/in/aggarwal-aman)\n",
    "\n",
    "Aman Aggarwal is a PhD working at the intersection of neuroscience, AI, and drug discovery. He specializes in quantitative microscopy and image processing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e156172-bf48-4869-b01b-4dda220188a0",
   "metadata": {},
   "source": [
    "<!--\n",
    "## Change Log\n",
    "\n",
    "'''|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "```\n",
    "```|---|---|---|---|\n",
    "```\n",
    "```| 2025-06-21  | 1.0  | Aman  |  Created the lab |\n",
    "```\n",
    "```| 2025-06-30  | 2.0  | Sangeeta |  ID review |\n",
    "```\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac1fc1b-0bbd-45e2-9a6a-f06b7b0ced04",
   "metadata": {},
   "source": [
    "© Copyright IBM Corporation. All rights reserved.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "fd4028941667565cdca5d946adb49321584044cacb8975f859bc4f258d5b52fa"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
